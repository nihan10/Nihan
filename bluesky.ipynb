{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6fc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atproto import Client\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Iterable, Optional\n",
    "import os, csv, time, requests\n",
    "\n",
    "# ---------- CONFIG / LOGIN ----------\n",
    "client = Client()\n",
    "client.login(\"YOUR USERNAME.bsky.social\", \"YOUR PASSWORD\")  #!!! In order to run this code, use your own user name and password for production\n",
    "\n",
    "# Where to save results (macOS)\n",
    "SAVE_DIR = \"/Users/,,,/Dekstop\"  ##!!!! Use your own path to save the file in\n",
    "\n",
    "# HTTP fallback base (public AppView)\n",
    "APPVIEW_BASE = \"https://public.api.bsky.app\"\n",
    "\n",
    "# ---------- COMPANIES & ALIASES ----------\n",
    "fortune_20 = {\n",
    "    1: \"Walmart\", 2: \"Amazon\", 3: \"Apple\", 4: \"CVS\", 5: \"Tesla\",\n",
    "    6: \"Google\", 7: \"Meta\", 8: \"JPMorgan\", 9: \"Costco\", 10: \"Kroger\",\n",
    "    11: \"Berkshire\", 12: \"Walgreens\", 13: \"Target\", 14: \"UPS\",\n",
    "    15: \"Centene\", 16: \"Cigna\", 17: \"Microsoft\", 18: \"Verizon\",\n",
    "    19: \"IBM\", 20: \"UnitedHealth\"\n",
    "}\n",
    "COMPANIES = list(fortune_20.values())\n",
    "\n",
    "# Add this dict (aliases + ticker forms). Multi-word terms are auto-quoted by your _normalize_term.\n",
    "COMPANY_ALIASES = {\n",
    "    \"Walmart\": [\n",
    "        \"Walmart\", \"#Walmart\"\n",
    "    ],\n",
    "    \"Amazon\": [\n",
    "        \"Amazon\", \"AMZN\", \"$AMZN\", \"#AMZN\", \"Amazon.com\", \"Jeff Bezos\"\n",
    "    ],\n",
    "    \"Apple\": [\n",
    "        \"Apple\", \"AAPL\", \"$AAPL\", \"#AAPL\", \"Apple Inc\", \"Tim Cook\"\n",
    "    ],\n",
    "    \"CVS\": [\n",
    "        \"CVS\", \"CVS Health\", \"CVS Pharmacy\", \"CVS Health Corp\", \"CVS Health Corporation\",\n",
    "        \"CVS\", \"$CVS\", \"#CVS\"\n",
    "    ],\n",
    "    \"Tesla\": [\n",
    "        \"Tesla\", \"TSLA\", \"$TSLA\", \"#TSLA\", \"Tesla Motors\", \"Elon Musk\"\n",
    "    ],\n",
    "    \"Google\": [\n",
    "        \"Google\", \"Alphabet Inc\", \"GOOGL\", \"$GOOGL\", \"#GOOGL\"\n",
    "    ],\n",
    "    \"Meta\": [\n",
    "        \"Meta\", \"Meta Platforms\", \"META\", \"$META\", \"#META\",\n",
    "        \"Facebook\", \"Mark Zuckerberg\"  # legacy but still used\n",
    "    ],\n",
    "    \"JPMorgan\": [\n",
    "        \"JPMorgan\", \"JP Morgan\", \"JPMorgan Chase\", \"JPMorgan Chase & Co.\",\n",
    "        \"JPM\", \"$JPM\", \"#JPM\", \"Chase\", \"#Chase\"\n",
    "    ],\n",
    "    \"Costco\": [\n",
    "        \"Costco\", \"Costco Wholesale\"\n",
    "    ],\n",
    "    \"Kroger\": [\n",
    "        \"Kroger\", \"The Kroger Co.\"\n",
    "    ],\n",
    "    \"Berkshire\": [\n",
    "        \"Berkshire\", \"Berkshire Hathaway\", \"Warren Buffett\"\n",
    "    ],\n",
    "    \"Walgreens\": [\n",
    "        \"Walgreens\"\n",
    "    ],\n",
    "    \"Target\": [\n",
    "        \"Target\"\n",
    "    ],\n",
    "    \"UPS\": [\n",
    "        \"UPS\", \"United Parcel Service\", \"$UPS\", \"#UPS\"\n",
    "    ],\n",
    "    \"Centene\": [\n",
    "        \"Centene\", \"Centene Corp\", \"Centene Corporation\"\n",
    "    ],\n",
    "    \"Cigna\": [\n",
    "        \"Cigna\", \"The Cigna Group\"\n",
    "    ],\n",
    "    \"Microsoft\": [\n",
    "        \"Microsoft\", \"MSFT\", \"$MSFT\", \"#MSFT\"\n",
    "    ],\n",
    "    \"Verizon\": [\n",
    "        \"Verizon\", \"Verizon Communications\"\n",
    "    ],\n",
    "    \"IBM\": [\n",
    "        \"IBM\", \"International Business Machines\", \"$IBM\", \"#IBM\"\n",
    "    ],\n",
    "    \"UnitedHealth\": [\n",
    "        \"UnitedHealth\", \"UnitedHealth Group\", \"United Healthcare\", \"UnitedHealthcare\",\n",
    "        \"UNH\", \"$UNH\", \"#UNH\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ---------- HELPERS ----------\n",
    "def _parse_iso_created_at(maybe_iso: str) -> Optional[datetime]:\n",
    "    if not maybe_iso:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.fromisoformat(maybe_iso.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _post_created_at(post) -> Optional[datetime]:\n",
    "    try:\n",
    "        rec = getattr(post, \"record\", None)\n",
    "        if rec:\n",
    "            for name in (\"created_at\", \"createdAt\"):\n",
    "                val = getattr(rec, name, None)\n",
    "                if val:\n",
    "                    dt = _parse_iso_created_at(val)\n",
    "                    if dt:\n",
    "                        return dt\n",
    "    except Exception:\n",
    "        pass\n",
    "    for name in (\"created_at\", \"createdAt\"):\n",
    "        val = getattr(post, name, None)\n",
    "        if val:\n",
    "            dt = _parse_iso_created_at(val)\n",
    "            if dt:\n",
    "                return dt\n",
    "    return None\n",
    "\n",
    "def _get_post_uri(obj) -> Optional[str]:\n",
    "    \"\"\"Return a stable unique id for dedupe (SDK: .uri, HTTP: ['uri']).\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return obj.get(\"uri\")\n",
    "    return getattr(obj, \"uri\", None)\n",
    "\n",
    "def _date_range_last_n_days(days: int, today: Optional[date] = None) -> List[date]:\n",
    "    if today is None:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "    return [(today - timedelta(days=i)) for i in range(days, 0, -1)]\n",
    "\n",
    "def _normalize_term(term: str) -> str:\n",
    "    \"\"\"Wrap multi-word terms in quotes if not already.\"\"\"\n",
    "    t = term.strip()\n",
    "    if \" \" in t and not (t.startswith('\"') and t.endswith('\"')):\n",
    "        t = f'\"{t}\"'\n",
    "    return t\n",
    "\n",
    "def _q_for_day(base_q: str, day_utc: date) -> str:\n",
    "    \"\"\"Inject UTC window into q so the API only returns that day.\"\"\"\n",
    "    day1 = day_utc.isoformat()\n",
    "    day2 = (day_utc + timedelta(days=1)).isoformat()\n",
    "    return f'{base_q} since:{day1} until:{day2}'\n",
    "\n",
    "# ---------- RESILIENT SEARCH ----------\n",
    "_RETRYABLE_HTTP = {429, 500, 502, 503, 504}\n",
    "\n",
    "def _sdk_search(q: str, limit: int, cursor: Optional[str]) -> object:\n",
    "    params = {\"q\": q, \"limit\": limit}\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "    return client.app.bsky.feed.search_posts(params)\n",
    "\n",
    "def _http_search(q: str, limit: int, cursor: Optional[str]) -> dict:\n",
    "    params = {\"q\": q, \"limit\": limit}\n",
    "    if cursor:\n",
    "        params[\"cursor\"] = cursor\n",
    "    r = requests.get(f\"{APPVIEW_BASE}/xrpc/app.bsky.feed.searchPosts\", params=params, timeout=30)\n",
    "    if r.status_code in _RETRYABLE_HTTP:\n",
    "        r.raise_for_status()\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _search_resilient(q: str, limit: int, cursor: Optional[str],\n",
    "                      sdk_tries: int = 3, http_tries: int = 3,\n",
    "                      backoff_base: float = 1.5):\n",
    "    \"\"\"\n",
    "    Try SDK with exponential backoff; on failure, fallback to HTTP with backoff.\n",
    "    Returns (posts, next_cursor, used_http: bool).\n",
    "    Posts are either SDK objects or dicts.\n",
    "    \"\"\"\n",
    "    # SDK\n",
    "    attempt = 0\n",
    "    while attempt < sdk_tries:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            resp = _sdk_search(q, limit, cursor)\n",
    "            posts = getattr(resp, \"posts\", []) or []\n",
    "            next_cur = getattr(resp, \"cursor\", None)\n",
    "            return posts, next_cur, False\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            if any(code in msg for code in (\"429\", \"500\", \"502\", \"503\", \"504\", \"UpstreamFailure\")):\n",
    "                time.sleep(min(backoff_base ** attempt, 30))\n",
    "                continue\n",
    "            break\n",
    "\n",
    "    # HTTP\n",
    "    attempt = 0\n",
    "    while attempt < http_tries:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            resp = _http_search(q, limit, cursor)\n",
    "            posts = (resp or {}).get(\"posts\") or []\n",
    "            next_cur = (resp or {}).get(\"cursor\")\n",
    "            return posts, next_cur, True\n",
    "        except requests.HTTPError as e:\n",
    "            status = getattr(e.response, \"status_code\", None)\n",
    "            if status in _RETRYABLE_HTTP:\n",
    "                time.sleep(min(backoff_base ** attempt, 30))\n",
    "                continue\n",
    "            raise\n",
    "        except requests.RequestException:\n",
    "            time.sleep(min(backoff_base ** attempt, 30))\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(\"Search failed after SDK and HTTP retries\")\n",
    "\n",
    "# ---------- CORE ----------\n",
    "def count_mentions_last_n_days_for_terms(\n",
    "    terms: List[str],\n",
    "    days: int = 365,\n",
    "    max_pages: int = 50,\n",
    "    per_request_limit: int = 100,\n",
    ") -> Counter:\n",
    "    \"\"\"\n",
    "    Count unique posts per day for a list of search terms (aliases).\n",
    "    - Queries each day with since:/until: in q\n",
    "    - Runs for each term, dedupes by post URI across terms and pages\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    today = datetime.now(timezone.utc).date()\n",
    "    day_list = _date_range_last_n_days(days, today)\n",
    "\n",
    "    # Normalize terms (quote multi-word)\n",
    "    terms = [_normalize_term(t) for t in terms]\n",
    "\n",
    "    for d in day_list:\n",
    "        start_dt = datetime.combine(d, datetime.min.time(), tzinfo=timezone.utc)\n",
    "        end_dt = datetime.combine(d + timedelta(days=1), datetime.min.time(), tzinfo=timezone.utc)\n",
    "\n",
    "        seen_uris: set = set()\n",
    "        daily_total = 0\n",
    "\n",
    "        for base in terms:\n",
    "            cursor = None\n",
    "            pages = 0\n",
    "            q_day = _q_for_day(base, d)\n",
    "\n",
    "            while True:\n",
    "                if pages >= max_pages:\n",
    "                    break\n",
    "                pages += 1\n",
    "\n",
    "                try:\n",
    "                    posts, cursor, used_http = _search_resilient(q_day, per_request_limit, cursor)\n",
    "                except Exception:\n",
    "                    # Give up this term/day, move to next term\n",
    "                    break\n",
    "\n",
    "                if not posts:\n",
    "                    break\n",
    "\n",
    "                first = posts[0]\n",
    "                is_dict = isinstance(first, dict)\n",
    "\n",
    "                if is_dict:\n",
    "                    # HTTP\n",
    "                    for post in posts:\n",
    "                        uri = post.get(\"uri\")\n",
    "                        if not uri or uri in seen_uris:\n",
    "                            continue\n",
    "                        rec = post.get(\"record\") or {}\n",
    "                        dt = _parse_iso_created_at(rec.get(\"createdAt\") or rec.get(\"created_at\"))\n",
    "                        if dt and (start_dt <= dt < end_dt):\n",
    "                            seen_uris.add(uri)\n",
    "                            daily_total += 1\n",
    "                else:\n",
    "                    # SDK\n",
    "                    for post in posts:\n",
    "                        uri = _get_post_uri(post)\n",
    "                        if not uri or uri in seen_uris:\n",
    "                            continue\n",
    "                        dt = _post_created_at(post)\n",
    "                        if dt and (start_dt <= dt < end_dt):\n",
    "                            seen_uris.add(uri)\n",
    "                            daily_total += 1\n",
    "\n",
    "                if not cursor:\n",
    "                    break\n",
    "\n",
    "        counts[d] = daily_total\n",
    "\n",
    "    return counts\n",
    "\n",
    "def counts_for_companies(\n",
    "    companies: Iterable[str],\n",
    "    days: int = 365\n",
    ") -> Dict[str, Counter]:\n",
    "    out: Dict[str, Counter] = {}\n",
    "    for name in companies:\n",
    "        terms = COMPANY_ALIASES.get(name, [name])\n",
    "        out[name] = count_mentions_last_n_days_for_terms(terms, days=days)\n",
    "    return out\n",
    "\n",
    "def write_company_date_matrix_csv(\n",
    "    company_to_counts: Dict[str, Counter],\n",
    "    days: int,\n",
    "    save_dir: str,\n",
    "    filename_prefix: str = \"bluesky_counts_matrix\"\n",
    ") -> str:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    today = datetime.now(timezone.utc).date()\n",
    "    start_day = today - timedelta(days=days)\n",
    "\n",
    "    dates = _date_range_last_n_days(days, today)\n",
    "    header = [\"company\"] + [d.isoformat() for d in dates]\n",
    "\n",
    "    filename = f\"{filename_prefix}_{start_day.strftime('%Y%m%d')}-{today.strftime('%Y%m%d')}.csv\"\n",
    "    path = os.path.join(save_dir, filename)\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        for company, cnt in company_to_counts.items():\n",
    "            row = [company] + [cnt.get(d, 0) for d in dates]\n",
    "            writer.writerow(row)\n",
    "\n",
    "    return path\n",
    "\n",
    "# ---------- RUN ----------\n",
    "if __name__ == \"__main__\":\n",
    "    DAYS = 365\n",
    "\n",
    "    company_counts = counts_for_companies(COMPANIES, days=DAYS)\n",
    "    csv_path = write_company_date_matrix_csv(company_counts, DAYS, SAVE_DIR)\n",
    "\n",
    "    print(f\"Saved matrix to: {csv_path}\\n\")\n",
    "\n",
    "    dates = _date_range_last_n_days(DAYS)\n",
    "    print(\"company,\", \", \".join(d.isoformat() for d in dates))\n",
    "    for c in COMPANIES:\n",
    "        cnt = company_counts.get(c, Counter())\n",
    "        row = [str(cnt.get(d, 0)) for d in dates]\n",
    "        print(f\"{c}, \" + \", \".join(row))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
